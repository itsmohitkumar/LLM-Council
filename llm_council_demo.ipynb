{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Council - Backend Demo\n",
    "\n",
    "This notebook demonstrates how to run the LLM Council backend code directly without needing the FastAPI server or React frontend.\n",
    "\n",
    "## What is LLM Council?\n",
    "\n",
    "LLM Council runs a 3-stage process:\n",
    "1. **Stage 1**: Multiple LLMs provide individual responses to your question\n",
    "2. **Stage 2**: Each LLM ranks the other responses (anonymized)\n",
    "3. **Stage 3**: A \"Chairman\" LLM synthesizes all responses into a final answer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "Run this cell to install all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install fastapi uvicorn python-dotenv httpx pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment Variables\n",
    "\n",
    "**IMPORTANT**: You need a Groq API key to use this notebook.\n",
    "\n",
    "1. Get your free API key from: https://console.groq.com/keys\n",
    "2. Replace `\"your_groq_api_key_here\"` below with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Groq API key here\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\"\n",
    "\n",
    "# Verify it's set\n",
    "if os.environ[\"GROQ_API_KEY\"] == \"your_groq_api_key_here\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Please replace 'your_groq_api_key_here' with your actual Groq API key!\")\n",
    "else:\n",
    "    print(\"‚úì API key is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Backend Modules\n",
    "\n",
    "Load all the LLM Council backend code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import backend modules\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "from backend.council import (\n",
    "    stage1_collect_responses,\n",
    "    stage2_collect_rankings,\n",
    "    stage3_synthesize_final,\n",
    "    calculate_aggregate_rankings,\n",
    "    run_full_council\n",
    ")\n",
    "from backend.config import COUNCIL_MODELS, CHAIRMAN_MODEL\n",
    "\n",
    "print(\"‚úì Backend modules imported successfully\")\n",
    "print(f\"\\nCouncil Members: {COUNCIL_MODELS}\")\n",
    "print(f\"Chairman Model: {CHAIRMAN_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Your Question\n",
    "\n",
    "Enter the question you want to ask the LLM Council:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your question for the council\n",
    "user_query = \"What are the key differences between supervised and unsupervised machine learning?\"\n",
    "\n",
    "print(f\"Question: {user_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Complete Council Process\n",
    "\n",
    "This will run all 3 stages automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Run the full council process\n",
    "print(\"üèõÔ∏è  Starting LLM Council...\\n\")\n",
    "\n",
    "stage1_results, stage2_results, stage3_result, metadata = await run_full_council(user_query)\n",
    "\n",
    "print(\"‚úì Council process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Stage 1 Results (Individual Responses)\n",
    "\n",
    "See what each LLM said individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: INDIVIDUAL RESPONSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(stage1_results, 1):\n",
    "    print(f\"\\n[{i}] Model: {result['model']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result['response'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: View Stage 2 Results (Peer Rankings)\n",
    "\n",
    "See how each LLM ranked the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: PEER RANKINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show the anonymous label mapping\n",
    "print(\"\\nüìã Anonymous Response Labels:\")\n",
    "for label, model in metadata['label_to_model'].items():\n",
    "    print(f\"  {label} = {model}\")\n",
    "\n",
    "# Show each model's ranking\n",
    "for i, result in enumerate(stage2_results, 1):\n",
    "    print(f\"\\n[{i}] Ranking by: {result['model']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result['ranking'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: View Aggregate Rankings\n",
    "\n",
    "See the overall consensus on which responses were best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"AGGREGATE RANKINGS (Lower average rank = Better)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, ranking in enumerate(metadata['aggregate_rankings'], 1):\n",
    "    print(f\"{i}. {ranking['model']}\")\n",
    "    print(f\"   Average Rank: {ranking['average_rank']}\")\n",
    "    print(f\"   Times Ranked: {ranking['rankings_count']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View Stage 3 Result (Final Synthesis)\n",
    "\n",
    "See the Chairman's final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 3: FINAL SYNTHESIS BY CHAIRMAN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nChairman Model: {stage3_result['model']}\")\n",
    "print(\"-\" * 80)\n",
    "print(stage3_result['response'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: Run Stages Individually\n",
    "\n",
    "If you want more control, you can run each stage separately:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Collect Individual Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 1 only\n",
    "custom_query = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "print(f\"Question: {custom_query}\\n\")\n",
    "stage1 = await stage1_collect_responses(custom_query)\n",
    "\n",
    "for i, result in enumerate(stage1, 1):\n",
    "    print(f\"\\n[{i}] {result['model']}:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(result['response'][:200] + \"...\")  # Show first 200 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Collect Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2 on the Stage 1 results\n",
    "stage2, label_map = await stage2_collect_rankings(custom_query, stage1)\n",
    "\n",
    "print(\"Label Mapping:\")\n",
    "for label, model in label_map.items():\n",
    "    print(f\"  {label} = {model}\")\n",
    "\n",
    "print(\"\\nParsed Rankings:\")\n",
    "for result in stage2:\n",
    "    print(f\"  {result['model']}: {result['parsed_ranking']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Synthesize Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 3 to get final synthesis\n",
    "stage3 = await stage3_synthesize_final(custom_query, stage1, stage2)\n",
    "\n",
    "print(f\"Chairman ({stage3['model']}) says:\\n\")\n",
    "print(stage3['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Test with Different Questions\n",
    "\n",
    "Try asking different types of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions you can try:\n",
    "example_questions = [\n",
    "    \"What is the meaning of life?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"What are the pros and cons of remote work?\",\n",
    "    \"Explain blockchain technology to a 10-year-old.\",\n",
    "    \"What makes a good software engineer?\"\n",
    "]\n",
    "\n",
    "# Pick one and run it through the council\n",
    "test_query = example_questions[1]  # Change the index to try different questions\n",
    "\n",
    "print(f\"Testing with: {test_query}\\n\")\n",
    "results = await run_full_council(test_query)\n",
    "\n",
    "# Show only the final answer\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(results[2]['response'])  # results[2] is stage3_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates the LLM Council backend functionality:\n",
    "\n",
    "1. ‚úÖ Stage 1: Collect responses from multiple LLMs\n",
    "2. ‚úÖ Stage 2: Have each LLM rank the responses\n",
    "3. ‚úÖ Stage 3: Synthesize final answer with a Chairman LLM\n",
    "\n",
    "**Note**: The full web application (with FastAPI backend + React frontend) provides a better user experience with:\n",
    "- Nice UI for viewing responses in tabs\n",
    "- Conversation history\n",
    "- Real-time streaming updates\n",
    "\n",
    "To run the full application, use the `./start.sh` script as described in the README.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
